{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!source activate tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Training--\n",
      "Vocabulary Size:  2636\n",
      "Step:0/10262, training_loss:165.053009\n",
      "Step:10/10262, training_loss:129.143997\n",
      "Step:20/10262, training_loss:127.318405\n",
      "Step:30/10262, training_loss:119.961685\n",
      "Step:40/10262, training_loss:129.355988\n",
      "Step:50/10262, training_loss:116.192245\n",
      "Step:60/10262, training_loss:133.710251\n",
      "Step:70/10262, training_loss:124.319138\n",
      "Step:80/10262, training_loss:124.762527\n",
      "Step:90/10262, training_loss:122.023468\n",
      "Step:100/10262, training_loss:125.603256\n",
      "Step:110/10262, training_loss:123.845840\n",
      "Step:120/10262, training_loss:112.508209\n",
      "Step:130/10262, training_loss:125.296661\n",
      "Step:140/10262, training_loss:125.311096\n",
      "Step:150/10262, training_loss:122.390907\n",
      "Step:160/10262, training_loss:138.290924\n",
      "Step:170/10262, training_loss:124.643822\n",
      "Step:180/10262, training_loss:137.258453\n",
      "Step:190/10262, training_loss:123.067551\n",
      "Step:200/10262, training_loss:113.624374\n",
      "Step:210/10262, training_loss:126.558838\n",
      "Step:220/10262, training_loss:117.018478\n",
      "Step:230/10262, training_loss:133.376343\n",
      "Step:240/10262, training_loss:128.478088\n",
      "Step:250/10262, training_loss:130.239365\n",
      "Step:260/10262, training_loss:124.831787\n",
      "Step:270/10262, training_loss:120.624130\n",
      "Step:280/10262, training_loss:125.000290\n",
      "Step:290/10262, training_loss:114.284714\n",
      "Step:300/10262, training_loss:115.785004\n",
      "Step:310/10262, training_loss:125.130753\n",
      "Step:320/10262, training_loss:125.712830\n",
      "Step:330/10262, training_loss:124.817261\n",
      "Step:340/10262, training_loss:123.788620\n",
      "Step:350/10262, training_loss:119.857094\n",
      "Step:360/10262, training_loss:119.653595\n",
      "Step:370/10262, training_loss:140.790741\n",
      "Step:380/10262, training_loss:110.924316\n",
      "Step:390/10262, training_loss:124.427017\n",
      "Step:400/10262, training_loss:125.094711\n",
      "Step:410/10262, training_loss:126.733345\n",
      "Step:420/10262, training_loss:113.892929\n",
      "Step:430/10262, training_loss:121.036911\n",
      "Step:440/10262, training_loss:114.199692\n",
      "Step:450/10262, training_loss:124.384857\n",
      "Step:460/10262, training_loss:113.204590\n",
      "Step:470/10262, training_loss:124.178337\n",
      "Step:480/10262, training_loss:115.318138\n",
      "Step:490/10262, training_loss:118.724693\n",
      "Step:500/10262, training_loss:114.787460\n",
      "Step:510/10262, training_loss:120.839188\n",
      "Step:520/10262, training_loss:118.349205\n",
      "Step:530/10262, training_loss:106.254288\n",
      "Step:540/10262, training_loss:116.525719\n",
      "Step:550/10262, training_loss:119.015968\n",
      "Step:560/10262, training_loss:114.291603\n",
      "Step:570/10262, training_loss:125.612923\n",
      "Step:580/10262, training_loss:117.127083\n",
      "Step:590/10262, training_loss:119.869026\n",
      "Step:600/10262, training_loss:115.533562\n",
      "Step:610/10262, training_loss:111.589790\n",
      "Step:620/10262, training_loss:114.204338\n",
      "Step:630/10262, training_loss:110.483398\n",
      "Step:640/10262, training_loss:125.013855\n",
      "Step:650/10262, training_loss:112.812614\n",
      "Step:660/10262, training_loss:115.182419\n",
      "Step:670/10262, training_loss:114.982735\n",
      "Step:680/10262, training_loss:112.757736\n",
      "Step:690/10262, training_loss:124.250290\n",
      "Step:700/10262, training_loss:112.085922\n",
      "Step:710/10262, training_loss:101.324997\n",
      "Step:720/10262, training_loss:116.147881\n",
      "Step:730/10262, training_loss:117.503891\n",
      "Step:740/10262, training_loss:108.705879\n",
      "Step:750/10262, training_loss:114.815430\n",
      "Step:760/10262, training_loss:114.562271\n",
      "Step:770/10262, training_loss:110.851158\n",
      "Step:780/10262, training_loss:116.662514\n",
      "Step:790/10262, training_loss:110.067963\n",
      "Step:800/10262, training_loss:112.619301\n",
      "Step:810/10262, training_loss:107.993515\n",
      "Step:820/10262, training_loss:114.663071\n",
      "Step:830/10262, training_loss:117.982491\n",
      "Step:840/10262, training_loss:113.999092\n",
      "Step:850/10262, training_loss:110.330994\n",
      "Step:860/10262, training_loss:114.928726\n",
      "Step:870/10262, training_loss:115.353142\n",
      "Step:880/10262, training_loss:120.792892\n",
      "Step:890/10262, training_loss:109.034515\n",
      "Step:900/10262, training_loss:113.214340\n",
      "Step:910/10262, training_loss:109.002457\n",
      "Step:920/10262, training_loss:122.878395\n",
      "Step:930/10262, training_loss:108.838966\n",
      "Step:940/10262, training_loss:111.571274\n",
      "Step:950/10262, training_loss:101.612923\n",
      "Step:960/10262, training_loss:110.073746\n",
      "Step:970/10262, training_loss:112.848244\n",
      "Step:980/10262, training_loss:112.029015\n",
      "Step:990/10262, training_loss:109.580841\n",
      "Step:1000/10262, training_loss:106.063354\n",
      "Step:1010/10262, training_loss:113.526108\n",
      "Step:1020/10262, training_loss:110.205223\n",
      "Step:1030/10262, training_loss:106.754776\n",
      "Step:1040/10262, training_loss:105.144394\n",
      "Step:1050/10262, training_loss:122.333694\n",
      "Step:1060/10262, training_loss:106.997787\n",
      "Step:1070/10262, training_loss:107.032898\n",
      "Step:1080/10262, training_loss:104.983734\n",
      "Step:1090/10262, training_loss:108.415672\n",
      "Step:1100/10262, training_loss:112.179977\n",
      "Step:1110/10262, training_loss:109.715393\n",
      "Step:1120/10262, training_loss:105.268173\n",
      "Step:1130/10262, training_loss:106.625832\n",
      "Step:1140/10262, training_loss:113.221451\n",
      "Step:1150/10262, training_loss:98.083679\n",
      "Step:1160/10262, training_loss:110.020859\n",
      "Step:1170/10262, training_loss:104.063606\n",
      "Step:1180/10262, training_loss:105.704651\n",
      "Step:1190/10262, training_loss:104.963638\n",
      "Step:1200/10262, training_loss:112.645752\n",
      "Step:1210/10262, training_loss:105.024414\n",
      "Step:1220/10262, training_loss:95.762558\n",
      "Step:1230/10262, training_loss:115.175751\n",
      "Step:1240/10262, training_loss:107.296684\n",
      "Step:1250/10262, training_loss:109.912704\n",
      "Step:1260/10262, training_loss:108.621971\n",
      "Step:1270/10262, training_loss:111.427628\n",
      "Step:1280/10262, training_loss:113.811844\n",
      "Step:1290/10262, training_loss:111.512756\n",
      "Step:1300/10262, training_loss:103.266464\n",
      "Step:1310/10262, training_loss:106.991486\n",
      "Step:1320/10262, training_loss:104.792686\n",
      "Step:1330/10262, training_loss:119.590698\n",
      "Step:1340/10262, training_loss:103.574127\n",
      "Step:1350/10262, training_loss:110.491013\n",
      "Step:1360/10262, training_loss:89.636108\n",
      "Step:1370/10262, training_loss:101.307053\n",
      "Step:1380/10262, training_loss:108.421791\n",
      "Step:1390/10262, training_loss:99.395042\n",
      "Step:1400/10262, training_loss:104.160461\n",
      "Step:1410/10262, training_loss:103.767090\n",
      "Step:1420/10262, training_loss:109.397446\n",
      "Step:1430/10262, training_loss:100.173943\n",
      "Step:1440/10262, training_loss:100.003578\n",
      "Step:1450/10262, training_loss:99.339752\n",
      "Step:1460/10262, training_loss:103.711601\n",
      "Step:1470/10262, training_loss:100.461136\n",
      "Step:1480/10262, training_loss:105.345642\n",
      "Step:1490/10262, training_loss:93.129074\n",
      "Step:1500/10262, training_loss:103.227798\n",
      "Step:1510/10262, training_loss:101.454674\n",
      "Step:1520/10262, training_loss:95.993065\n",
      "Step:1530/10262, training_loss:105.252792\n",
      "Step:1540/10262, training_loss:95.035423\n",
      "Step:1550/10262, training_loss:98.097351\n",
      "Step:1560/10262, training_loss:90.727875\n",
      "Step:1570/10262, training_loss:103.591072\n",
      "Step:1580/10262, training_loss:94.826843\n",
      "Step:1590/10262, training_loss:94.402328\n",
      "Step:1600/10262, training_loss:101.880318\n",
      "Step:1610/10262, training_loss:103.664665\n",
      "Step:1620/10262, training_loss:93.096733\n",
      "Step:1630/10262, training_loss:91.687523\n",
      "Step:1640/10262, training_loss:105.604744\n",
      "Step:1650/10262, training_loss:95.183868\n",
      "Step:1660/10262, training_loss:100.178940\n",
      "Step:1670/10262, training_loss:99.215530\n",
      "Step:1680/10262, training_loss:102.140244\n",
      "Step:1690/10262, training_loss:97.070541\n",
      "Step:1700/10262, training_loss:104.142120\n",
      "Step:1710/10262, training_loss:100.389069\n",
      "Step:1720/10262, training_loss:96.770111\n",
      "Step:1730/10262, training_loss:98.306252\n",
      "Step:1740/10262, training_loss:100.907684\n",
      "Step:1750/10262, training_loss:98.517044\n",
      "Step:1760/10262, training_loss:96.912445\n",
      "Step:1770/10262, training_loss:88.943146\n",
      "Step:1780/10262, training_loss:95.851624\n",
      "Step:1790/10262, training_loss:96.673950\n",
      "Step:1800/10262, training_loss:100.413689\n",
      "Step:1810/10262, training_loss:94.973679\n",
      "Step:1820/10262, training_loss:99.383270\n",
      "Step:1830/10262, training_loss:97.021873\n",
      "Step:1840/10262, training_loss:90.134216\n",
      "Step:1850/10262, training_loss:94.281586\n",
      "Step:1860/10262, training_loss:94.057861\n",
      "Step:1870/10262, training_loss:91.236420\n",
      "Step:1880/10262, training_loss:91.521935\n",
      "Step:1890/10262, training_loss:96.582756\n",
      "Step:1900/10262, training_loss:90.638565\n",
      "Step:1910/10262, training_loss:100.158577\n",
      "Step:1920/10262, training_loss:95.378876\n",
      "Step:1930/10262, training_loss:96.254303\n",
      "Step:1940/10262, training_loss:98.181221\n",
      "Step:1950/10262, training_loss:91.100273\n",
      "Step:1960/10262, training_loss:82.492714\n",
      "Step:1970/10262, training_loss:92.602150\n",
      "Step:1980/10262, training_loss:86.429092\n",
      "Step:1990/10262, training_loss:88.386719\n",
      "Step:2000/10262, training_loss:87.503654\n",
      "Step:2010/10262, training_loss:98.288605\n",
      "Step:2020/10262, training_loss:90.205994\n",
      "Step:2030/10262, training_loss:90.644028\n",
      "Step:2040/10262, training_loss:88.398117\n",
      "Step:2050/10262, training_loss:91.504669\n",
      "Step:2060/10262, training_loss:92.673523\n",
      "Step:2070/10262, training_loss:85.379402\n",
      "Step:2080/10262, training_loss:95.372971\n",
      "Step:2090/10262, training_loss:90.774170\n",
      "Step:2100/10262, training_loss:88.268837\n",
      "Step:2110/10262, training_loss:94.175919\n",
      "Step:2120/10262, training_loss:95.885620\n",
      "Step:2130/10262, training_loss:94.302773\n",
      "Step:2140/10262, training_loss:92.663589\n",
      "Step:2150/10262, training_loss:86.061310\n",
      "Step:2160/10262, training_loss:92.491409\n",
      "Step:2170/10262, training_loss:85.259155\n",
      "Step:2180/10262, training_loss:80.208679\n",
      "Step:2190/10262, training_loss:86.248199\n",
      "Step:2200/10262, training_loss:90.213181\n",
      "Step:2210/10262, training_loss:96.255516\n",
      "Step:2220/10262, training_loss:83.939728\n",
      "Step:2230/10262, training_loss:92.811737\n",
      "Step:2240/10262, training_loss:84.664093\n",
      "Step:2250/10262, training_loss:82.665802\n",
      "Step:2260/10262, training_loss:91.580894\n",
      "Step:2270/10262, training_loss:85.114868\n",
      "Step:2280/10262, training_loss:89.367569\n",
      "Step:2290/10262, training_loss:84.592743\n",
      "Step:2300/10262, training_loss:83.717125\n",
      "Step:2310/10262, training_loss:82.192688\n",
      "Step:2320/10262, training_loss:92.153214\n",
      "Step:2330/10262, training_loss:76.514870\n",
      "Step:2340/10262, training_loss:92.895027\n",
      "Step:2350/10262, training_loss:92.882568\n",
      "Step:2360/10262, training_loss:91.457558\n",
      "Step:2370/10262, training_loss:74.386391\n",
      "Step:2380/10262, training_loss:88.636566\n",
      "Step:2390/10262, training_loss:79.806030\n",
      "Step:2400/10262, training_loss:91.725334\n",
      "Step:2410/10262, training_loss:82.372925\n",
      "Step:2420/10262, training_loss:90.627159\n",
      "Step:2430/10262, training_loss:79.972824\n",
      "Step:2440/10262, training_loss:78.013733\n",
      "Step:2450/10262, training_loss:86.809158\n",
      "Step:2460/10262, training_loss:87.068985\n",
      "Step:2470/10262, training_loss:86.352455\n",
      "Step:2480/10262, training_loss:73.222763\n",
      "Step:2490/10262, training_loss:86.814598\n",
      "Step:2500/10262, training_loss:83.592819\n",
      "Step:2510/10262, training_loss:80.559273\n",
      "Step:2520/10262, training_loss:89.179245\n",
      "Step:2530/10262, training_loss:86.268448\n",
      "Step:2540/10262, training_loss:88.634094\n",
      "Step:2550/10262, training_loss:86.351730\n",
      "Step:2560/10262, training_loss:79.245560\n",
      "Step:2570/10262, training_loss:84.799355\n",
      "Step:2580/10262, training_loss:77.546555\n",
      "Step:2590/10262, training_loss:81.346107\n",
      "Step:2600/10262, training_loss:79.915390\n",
      "Step:2610/10262, training_loss:84.462738\n",
      "Step:2620/10262, training_loss:83.679657\n",
      "Step:2630/10262, training_loss:72.244629\n",
      "Step:2640/10262, training_loss:91.341629\n",
      "Step:2650/10262, training_loss:74.936592\n",
      "Step:2660/10262, training_loss:72.017586\n",
      "Step:2670/10262, training_loss:86.093216\n",
      "Step:2680/10262, training_loss:76.096008\n",
      "Step:2690/10262, training_loss:77.992264\n",
      "Step:2700/10262, training_loss:74.665154\n",
      "Step:2710/10262, training_loss:80.843628\n",
      "Step:2720/10262, training_loss:74.038445\n",
      "Step:2730/10262, training_loss:78.834946\n",
      "Step:2740/10262, training_loss:66.583145\n",
      "Step:2750/10262, training_loss:84.431557\n",
      "Step:2760/10262, training_loss:79.147125\n",
      "Step:2770/10262, training_loss:83.823059\n",
      "Step:2780/10262, training_loss:85.495316\n",
      "Step:2790/10262, training_loss:77.371704\n",
      "Step:2800/10262, training_loss:73.583076\n",
      "Step:2810/10262, training_loss:82.758972\n",
      "Step:2820/10262, training_loss:79.828751\n",
      "Step:2830/10262, training_loss:81.067078\n",
      "Step:2840/10262, training_loss:69.444977\n",
      "Step:2850/10262, training_loss:76.195068\n",
      "Step:2860/10262, training_loss:80.366394\n",
      "Step:2870/10262, training_loss:90.900253\n",
      "Step:2880/10262, training_loss:76.679703\n",
      "Step:2890/10262, training_loss:72.713997\n",
      "Step:2900/10262, training_loss:72.078804\n",
      "Step:2910/10262, training_loss:73.673813\n",
      "Step:2920/10262, training_loss:70.493637\n",
      "Step:2930/10262, training_loss:82.294540\n",
      "Step:2940/10262, training_loss:77.206978\n",
      "Step:2950/10262, training_loss:75.444580\n",
      "Step:2960/10262, training_loss:83.605896\n",
      "Step:2970/10262, training_loss:75.326469\n",
      "Step:2980/10262, training_loss:77.850693\n",
      "Step:2990/10262, training_loss:68.888748\n",
      "Step:3000/10262, training_loss:79.596664\n",
      "Step:3010/10262, training_loss:62.664246\n",
      "Step:3020/10262, training_loss:74.719025\n",
      "Step:3030/10262, training_loss:77.340950\n",
      "Step:3040/10262, training_loss:63.179054\n",
      "Step:3050/10262, training_loss:81.138489\n",
      "Step:3060/10262, training_loss:71.173561\n",
      "Step:3070/10262, training_loss:67.519180\n",
      "Step:3080/10262, training_loss:74.810341\n",
      "Step:3090/10262, training_loss:69.156845\n",
      "Step:3100/10262, training_loss:67.557678\n",
      "Step:3110/10262, training_loss:71.231628\n",
      "Step:3120/10262, training_loss:72.631447\n",
      "Step:3130/10262, training_loss:75.784637\n",
      "Step:3140/10262, training_loss:62.757244\n",
      "Step:3150/10262, training_loss:74.165581\n",
      "Step:3160/10262, training_loss:70.328293\n",
      "Step:3170/10262, training_loss:62.112183\n",
      "Step:3180/10262, training_loss:77.597435\n",
      "Step:3190/10262, training_loss:80.262680\n",
      "Step:3200/10262, training_loss:68.175262\n",
      "Step:3210/10262, training_loss:64.919731\n",
      "Step:3220/10262, training_loss:72.099808\n",
      "Step:3230/10262, training_loss:75.608391\n",
      "Step:3240/10262, training_loss:66.841690\n",
      "Step:3250/10262, training_loss:60.712936\n",
      "Step:3260/10262, training_loss:74.293930\n",
      "Step:3270/10262, training_loss:68.330994\n",
      "Step:3280/10262, training_loss:75.926750\n",
      "Step:3290/10262, training_loss:69.574303\n",
      "Step:3300/10262, training_loss:65.501785\n",
      "Step:3310/10262, training_loss:65.001343\n",
      "Step:3320/10262, training_loss:64.337608\n",
      "Step:3330/10262, training_loss:65.593887\n",
      "Step:3340/10262, training_loss:62.424175\n",
      "Step:3350/10262, training_loss:72.519409\n",
      "Step:3360/10262, training_loss:70.922768\n",
      "Step:3370/10262, training_loss:74.124649\n",
      "Step:3380/10262, training_loss:67.945915\n",
      "Step:3390/10262, training_loss:67.473755\n",
      "Step:3400/10262, training_loss:61.766048\n",
      "Step:3410/10262, training_loss:63.177799\n",
      "Step:3420/10262, training_loss:46.095390\n",
      "Step:3430/10262, training_loss:62.452400\n",
      "Step:3440/10262, training_loss:67.080109\n",
      "Step:3450/10262, training_loss:56.824154\n",
      "Step:3460/10262, training_loss:64.953857\n",
      "Step:3470/10262, training_loss:67.467941\n",
      "Step:3480/10262, training_loss:70.283981\n",
      "Step:3490/10262, training_loss:66.504303\n",
      "Step:3500/10262, training_loss:60.875931\n",
      "Step:3510/10262, training_loss:59.839439\n",
      "Step:3520/10262, training_loss:68.951561\n",
      "Step:3530/10262, training_loss:58.530708\n",
      "Step:3540/10262, training_loss:67.568214\n",
      "Step:3550/10262, training_loss:57.119335\n",
      "Step:3560/10262, training_loss:71.958313\n",
      "Step:3570/10262, training_loss:60.670219\n",
      "Step:3580/10262, training_loss:60.607994\n",
      "Step:3590/10262, training_loss:66.781540\n",
      "Step:3600/10262, training_loss:66.310196\n",
      "Step:3610/10262, training_loss:70.730034\n",
      "Step:3620/10262, training_loss:59.398743\n",
      "Step:3630/10262, training_loss:69.196655\n",
      "Step:3640/10262, training_loss:71.659821\n",
      "Step:3650/10262, training_loss:56.614517\n",
      "Step:3660/10262, training_loss:65.443253\n",
      "Step:3670/10262, training_loss:57.879723\n",
      "Step:3680/10262, training_loss:62.922470\n",
      "Step:3690/10262, training_loss:63.367203\n",
      "Step:3700/10262, training_loss:71.230377\n",
      "Step:3710/10262, training_loss:60.821884\n",
      "Step:3720/10262, training_loss:51.499519\n",
      "Step:3730/10262, training_loss:54.569809\n",
      "Step:3740/10262, training_loss:56.486408\n",
      "Step:3750/10262, training_loss:56.505352\n",
      "Step:3760/10262, training_loss:69.154404\n",
      "Step:3770/10262, training_loss:69.391350\n",
      "Step:3780/10262, training_loss:57.521629\n",
      "Step:3790/10262, training_loss:57.114033\n",
      "Step:3800/10262, training_loss:61.136333\n",
      "Step:3810/10262, training_loss:52.138721\n",
      "Step:3820/10262, training_loss:57.516624\n",
      "Step:3830/10262, training_loss:40.364513\n",
      "Step:3840/10262, training_loss:58.849648\n",
      "Step:3850/10262, training_loss:61.333633\n",
      "Step:3860/10262, training_loss:56.203648\n",
      "Step:3870/10262, training_loss:52.763313\n",
      "Step:3880/10262, training_loss:60.623943\n",
      "Step:3890/10262, training_loss:66.190613\n",
      "Step:3900/10262, training_loss:56.098640\n",
      "Step:3910/10262, training_loss:44.092621\n",
      "Step:3920/10262, training_loss:57.798172\n",
      "Step:3930/10262, training_loss:56.860344\n",
      "Step:3940/10262, training_loss:44.440426\n",
      "Step:3950/10262, training_loss:60.176094\n",
      "Step:3960/10262, training_loss:55.396938\n",
      "Step:3970/10262, training_loss:63.114624\n",
      "Step:3980/10262, training_loss:52.650703\n",
      "Step:3990/10262, training_loss:56.007805\n",
      "Step:4000/10262, training_loss:58.536461\n",
      "Step:4010/10262, training_loss:58.017029\n",
      "Step:4020/10262, training_loss:61.606911\n",
      "Step:4030/10262, training_loss:57.584423\n",
      "Step:4040/10262, training_loss:61.497063\n",
      "Step:4050/10262, training_loss:59.962074\n",
      "Step:4060/10262, training_loss:50.663261\n",
      "Step:4070/10262, training_loss:64.230072\n",
      "Step:4080/10262, training_loss:55.228188\n",
      "Step:4090/10262, training_loss:58.613197\n",
      "Step:4100/10262, training_loss:61.687656\n",
      "Step:4110/10262, training_loss:66.373459\n",
      "Step:4120/10262, training_loss:56.174408\n",
      "Step:4130/10262, training_loss:39.342117\n",
      "Step:4140/10262, training_loss:49.311100\n",
      "Step:4150/10262, training_loss:48.248158\n",
      "Step:4160/10262, training_loss:57.904522\n",
      "Step:4170/10262, training_loss:54.163391\n",
      "Step:4180/10262, training_loss:66.022720\n",
      "Step:4190/10262, training_loss:53.695938\n",
      "Step:4200/10262, training_loss:47.706245\n",
      "Step:4210/10262, training_loss:59.786819\n",
      "Step:4220/10262, training_loss:41.416763\n",
      "Step:4230/10262, training_loss:50.910412\n",
      "Step:4240/10262, training_loss:38.920177\n",
      "Step:4250/10262, training_loss:58.708565\n",
      "Step:4260/10262, training_loss:49.146271\n",
      "Step:4270/10262, training_loss:50.699577\n",
      "Step:4280/10262, training_loss:40.635414\n",
      "Step:4290/10262, training_loss:56.428787\n",
      "Step:4300/10262, training_loss:54.953400\n",
      "Step:4310/10262, training_loss:45.406555\n",
      "Step:4320/10262, training_loss:36.797203\n",
      "Step:4330/10262, training_loss:64.398117\n",
      "Step:4340/10262, training_loss:53.175499\n",
      "Step:4350/10262, training_loss:56.975895\n",
      "Step:4360/10262, training_loss:54.032604\n",
      "Step:4370/10262, training_loss:49.617119\n",
      "Step:4380/10262, training_loss:49.647377\n",
      "Step:4390/10262, training_loss:42.605347\n",
      "Step:4400/10262, training_loss:51.592468\n",
      "Step:4410/10262, training_loss:51.923775\n",
      "Step:4420/10262, training_loss:51.202847\n",
      "Step:4430/10262, training_loss:47.618698\n",
      "Step:4440/10262, training_loss:58.137753\n",
      "Step:4450/10262, training_loss:52.587406\n",
      "Step:4460/10262, training_loss:51.674835\n",
      "Step:4470/10262, training_loss:51.304440\n",
      "Step:4480/10262, training_loss:57.833870\n",
      "Step:4490/10262, training_loss:54.502899\n",
      "Step:4500/10262, training_loss:55.160378\n",
      "Step:4510/10262, training_loss:50.893089\n",
      "Step:4520/10262, training_loss:55.831879\n",
      "Step:4530/10262, training_loss:48.808945\n",
      "Step:4540/10262, training_loss:43.206696\n",
      "Step:4550/10262, training_loss:47.915009\n",
      "Step:4560/10262, training_loss:48.347862\n",
      "Step:4570/10262, training_loss:61.844231\n",
      "Step:4580/10262, training_loss:38.478149\n",
      "Step:4590/10262, training_loss:59.715950\n",
      "Step:4600/10262, training_loss:45.320000\n",
      "Step:4610/10262, training_loss:46.639709\n",
      "Step:4620/10262, training_loss:52.553806\n",
      "Step:4630/10262, training_loss:39.434158\n",
      "Step:4640/10262, training_loss:45.187359\n",
      "Step:4650/10262, training_loss:33.795464\n",
      "Step:4660/10262, training_loss:58.764713\n",
      "Step:4670/10262, training_loss:43.388405\n",
      "Step:4680/10262, training_loss:45.205921\n",
      "Step:4690/10262, training_loss:36.834202\n",
      "Step:4700/10262, training_loss:55.840233\n",
      "Step:4710/10262, training_loss:51.269562\n",
      "Step:4720/10262, training_loss:56.441807\n",
      "Step:4730/10262, training_loss:54.627316\n",
      "Step:4740/10262, training_loss:52.026356\n",
      "Step:4750/10262, training_loss:46.801704\n",
      "Step:4760/10262, training_loss:59.265007\n",
      "Step:4770/10262, training_loss:48.689568\n",
      "Step:4780/10262, training_loss:50.884342\n",
      "Step:4790/10262, training_loss:36.405632\n",
      "Step:4800/10262, training_loss:49.620159\n",
      "Step:4810/10262, training_loss:49.531460\n",
      "Step:4820/10262, training_loss:54.334694\n",
      "Step:4830/10262, training_loss:45.105583\n",
      "Step:4840/10262, training_loss:38.917641\n",
      "Step:4850/10262, training_loss:48.481106\n",
      "Step:4860/10262, training_loss:43.865116\n",
      "Step:4870/10262, training_loss:42.950966\n",
      "Step:4880/10262, training_loss:55.716915\n",
      "Step:4890/10262, training_loss:52.276360\n",
      "Step:4900/10262, training_loss:51.185585\n",
      "Step:4910/10262, training_loss:56.048729\n",
      "Step:4920/10262, training_loss:48.152344\n",
      "Step:4930/10262, training_loss:51.156464\n",
      "Step:4940/10262, training_loss:41.446404\n",
      "Step:4950/10262, training_loss:44.895069\n",
      "Step:4960/10262, training_loss:35.777771\n",
      "Step:4970/10262, training_loss:45.387146\n",
      "Step:4980/10262, training_loss:60.982262\n",
      "Step:4990/10262, training_loss:29.716417\n",
      "Step:5000/10262, training_loss:52.443848\n",
      "Step:5010/10262, training_loss:43.119270\n",
      "Step:5020/10262, training_loss:42.930706\n",
      "Step:5030/10262, training_loss:49.364925\n",
      "Step:5040/10262, training_loss:37.367340\n",
      "Step:5050/10262, training_loss:45.925171\n",
      "Step:5060/10262, training_loss:37.201431\n",
      "Step:5070/10262, training_loss:51.347973\n",
      "Step:5080/10262, training_loss:50.451847\n",
      "Step:5090/10262, training_loss:37.808250\n",
      "Step:5100/10262, training_loss:47.147942\n",
      "Step:5110/10262, training_loss:47.549919\n",
      "Step:5120/10262, training_loss:45.144691\n",
      "Step:5130/10262, training_loss:54.820511\n",
      "Step:5140/10262, training_loss:62.646461\n",
      "Step:5150/10262, training_loss:38.865402\n",
      "Step:5160/10262, training_loss:39.011917\n",
      "Step:5170/10262, training_loss:45.879890\n",
      "Step:5180/10262, training_loss:48.808060\n",
      "Step:5190/10262, training_loss:43.909126\n",
      "Step:5200/10262, training_loss:36.919971\n",
      "Step:5210/10262, training_loss:55.174370\n",
      "Step:5220/10262, training_loss:44.617443\n",
      "Step:5230/10262, training_loss:45.773823\n",
      "Step:5240/10262, training_loss:44.424706\n",
      "Step:5250/10262, training_loss:37.336250\n",
      "Step:5260/10262, training_loss:40.784760\n",
      "Step:5270/10262, training_loss:44.599464\n",
      "Step:5280/10262, training_loss:35.985619\n",
      "Step:5290/10262, training_loss:42.135365\n",
      "Step:5300/10262, training_loss:53.808475\n",
      "Step:5310/10262, training_loss:46.651695\n",
      "Step:5320/10262, training_loss:49.967430\n",
      "Step:5330/10262, training_loss:47.230942\n",
      "Step:5340/10262, training_loss:43.308887\n",
      "Step:5350/10262, training_loss:38.254318\n",
      "Step:5360/10262, training_loss:39.188133\n",
      "Step:5370/10262, training_loss:23.563761\n",
      "Step:5380/10262, training_loss:37.278313\n",
      "Step:5390/10262, training_loss:51.115669\n",
      "Step:5400/10262, training_loss:31.393286\n",
      "Step:5410/10262, training_loss:37.350868\n",
      "Step:5420/10262, training_loss:45.734451\n",
      "Step:5430/10262, training_loss:45.841518\n",
      "Step:5440/10262, training_loss:46.974613\n",
      "Step:5450/10262, training_loss:37.148834\n",
      "Step:5460/10262, training_loss:40.438778\n",
      "Step:5470/10262, training_loss:41.100426\n",
      "Step:5480/10262, training_loss:38.710625\n",
      "Step:5490/10262, training_loss:47.641228\n",
      "Step:5500/10262, training_loss:34.232494\n",
      "Step:5510/10262, training_loss:50.287075\n",
      "Step:5520/10262, training_loss:38.267403\n",
      "Step:5530/10262, training_loss:37.825203\n",
      "Step:5540/10262, training_loss:39.939674\n",
      "Step:5550/10262, training_loss:46.246700\n",
      "Step:5560/10262, training_loss:43.952911\n",
      "Step:5570/10262, training_loss:32.165634\n",
      "Step:5580/10262, training_loss:47.799911\n",
      "Step:5590/10262, training_loss:49.068890\n",
      "Step:5600/10262, training_loss:34.626701\n",
      "Step:5610/10262, training_loss:44.293655\n",
      "Step:5620/10262, training_loss:43.000847\n",
      "Step:5630/10262, training_loss:38.036240\n",
      "Step:5640/10262, training_loss:36.346764\n",
      "Step:5650/10262, training_loss:48.488152\n",
      "Step:5660/10262, training_loss:36.291859\n",
      "Step:5670/10262, training_loss:33.609680\n",
      "Step:5680/10262, training_loss:33.647087\n",
      "Step:5690/10262, training_loss:30.939407\n",
      "Step:5700/10262, training_loss:32.628418\n",
      "Step:5710/10262, training_loss:52.484699\n",
      "Step:5720/10262, training_loss:50.933243\n",
      "Step:5730/10262, training_loss:39.478432\n",
      "Step:5740/10262, training_loss:37.243187\n",
      "Step:5750/10262, training_loss:40.800175\n",
      "Step:5760/10262, training_loss:30.209932\n",
      "Step:5770/10262, training_loss:36.494320\n",
      "Step:5780/10262, training_loss:18.608419\n",
      "Step:5790/10262, training_loss:33.476753\n",
      "Step:5800/10262, training_loss:42.071190\n",
      "Step:5810/10262, training_loss:33.001244\n",
      "Step:5820/10262, training_loss:26.751295\n",
      "Step:5830/10262, training_loss:41.051743\n",
      "Step:5840/10262, training_loss:43.223717\n",
      "Step:5850/10262, training_loss:34.893463\n",
      "Step:5860/10262, training_loss:27.017056\n",
      "Step:5870/10262, training_loss:37.775928\n",
      "Step:5880/10262, training_loss:41.168068\n",
      "Step:5890/10262, training_loss:25.374084\n",
      "Step:5900/10262, training_loss:39.537361\n",
      "Step:5910/10262, training_loss:33.082462\n",
      "Step:5920/10262, training_loss:43.848724\n",
      "Step:5930/10262, training_loss:33.669533\n",
      "Step:5940/10262, training_loss:35.549347\n",
      "Step:5950/10262, training_loss:32.678352\n",
      "Step:5960/10262, training_loss:36.351250\n",
      "Step:5970/10262, training_loss:41.797623\n",
      "Step:5980/10262, training_loss:32.411266\n",
      "Step:5990/10262, training_loss:46.191299\n",
      "Step:6000/10262, training_loss:45.804848\n",
      "Step:6010/10262, training_loss:28.585865\n",
      "Step:6020/10262, training_loss:46.714317\n",
      "Step:6030/10262, training_loss:36.453690\n",
      "Step:6040/10262, training_loss:36.295315\n",
      "Step:6050/10262, training_loss:38.729324\n",
      "Step:6060/10262, training_loss:46.849155\n",
      "Step:6070/10262, training_loss:35.578030\n",
      "Step:6080/10262, training_loss:21.277927\n",
      "Step:6090/10262, training_loss:26.303238\n",
      "Step:6100/10262, training_loss:31.142414\n",
      "Step:6110/10262, training_loss:36.820301\n",
      "Step:6120/10262, training_loss:41.376282\n",
      "Step:6130/10262, training_loss:49.355850\n",
      "Step:6140/10262, training_loss:36.439598\n",
      "Step:6150/10262, training_loss:30.953297\n",
      "Step:6160/10262, training_loss:42.144905\n",
      "Step:6170/10262, training_loss:21.818928\n",
      "Step:6180/10262, training_loss:32.823051\n",
      "Step:6190/10262, training_loss:19.078979\n",
      "Step:6200/10262, training_loss:40.170506\n",
      "Step:6210/10262, training_loss:31.233795\n",
      "Step:6220/10262, training_loss:28.687321\n",
      "Step:6230/10262, training_loss:21.545090\n",
      "Step:6240/10262, training_loss:37.076782\n",
      "Step:6250/10262, training_loss:36.083244\n",
      "Step:6260/10262, training_loss:25.068214\n",
      "Step:6270/10262, training_loss:20.879398\n",
      "Step:6280/10262, training_loss:43.768101\n",
      "Step:6290/10262, training_loss:34.245724\n",
      "Step:6300/10262, training_loss:30.294592\n",
      "Step:6310/10262, training_loss:36.708866\n",
      "Step:6320/10262, training_loss:26.588058\n",
      "Step:6330/10262, training_loss:32.527615\n",
      "Step:6340/10262, training_loss:30.353374\n",
      "Step:6350/10262, training_loss:35.032181\n",
      "Step:6360/10262, training_loss:30.531849\n",
      "Step:6370/10262, training_loss:34.869804\n",
      "Step:6380/10262, training_loss:32.199421\n",
      "Step:6390/10262, training_loss:39.730247\n",
      "Step:6400/10262, training_loss:35.645206\n",
      "Step:6410/10262, training_loss:36.653191\n",
      "Step:6420/10262, training_loss:27.806608\n",
      "Step:6430/10262, training_loss:42.280090\n",
      "Step:6440/10262, training_loss:38.181465\n",
      "Step:6450/10262, training_loss:39.558083\n",
      "Step:6460/10262, training_loss:35.638577\n",
      "Step:6470/10262, training_loss:42.021282\n",
      "Step:6480/10262, training_loss:32.077152\n",
      "Step:6490/10262, training_loss:22.775406\n",
      "Step:6500/10262, training_loss:27.791906\n",
      "Step:6510/10262, training_loss:29.143198\n",
      "Step:6520/10262, training_loss:44.121063\n",
      "Step:6530/10262, training_loss:23.666344\n",
      "Step:6540/10262, training_loss:43.270401\n",
      "Step:6550/10262, training_loss:31.348181\n",
      "Step:6560/10262, training_loss:28.975719\n",
      "Step:6570/10262, training_loss:36.336853\n",
      "Step:6580/10262, training_loss:20.579494\n",
      "Step:6590/10262, training_loss:29.290678\n",
      "Step:6600/10262, training_loss:17.856276\n",
      "Step:6610/10262, training_loss:43.343849\n",
      "Step:6620/10262, training_loss:27.835007\n",
      "Step:6630/10262, training_loss:27.252434\n",
      "Step:6640/10262, training_loss:21.541992\n",
      "Step:6650/10262, training_loss:38.333710\n",
      "Step:6660/10262, training_loss:33.912712\n",
      "Step:6670/10262, training_loss:33.865356\n",
      "Step:6680/10262, training_loss:35.682671\n",
      "Step:6690/10262, training_loss:37.688301\n",
      "Step:6700/10262, training_loss:31.615822\n",
      "Step:6710/10262, training_loss:41.390282\n",
      "Step:6720/10262, training_loss:31.166323\n",
      "Step:6730/10262, training_loss:30.205029\n",
      "Step:6740/10262, training_loss:23.244766\n",
      "Step:6750/10262, training_loss:34.704792\n",
      "Step:6760/10262, training_loss:35.131096\n",
      "Step:6770/10262, training_loss:35.365631\n",
      "Step:6780/10262, training_loss:29.775347\n",
      "Step:6790/10262, training_loss:24.693302\n",
      "Step:6800/10262, training_loss:37.683136\n",
      "Step:6810/10262, training_loss:27.106468\n",
      "Step:6820/10262, training_loss:23.368275\n",
      "Step:6830/10262, training_loss:38.397781\n",
      "Step:6840/10262, training_loss:38.088909\n",
      "Step:6850/10262, training_loss:36.829174\n",
      "Step:6860/10262, training_loss:40.379463\n",
      "Step:6870/10262, training_loss:34.752419\n",
      "Step:6880/10262, training_loss:36.592342\n",
      "Step:6890/10262, training_loss:24.970345\n",
      "Step:6900/10262, training_loss:26.529997\n",
      "Step:6910/10262, training_loss:26.459347\n",
      "Step:6920/10262, training_loss:26.403812\n",
      "Step:6930/10262, training_loss:43.343307\n",
      "Step:6940/10262, training_loss:16.607151\n",
      "Step:6950/10262, training_loss:36.416946\n",
      "Step:6960/10262, training_loss:29.276161\n",
      "Step:6970/10262, training_loss:28.342159\n",
      "Step:6980/10262, training_loss:36.614571\n",
      "Step:6990/10262, training_loss:21.335346\n",
      "Step:7000/10262, training_loss:31.167219\n",
      "Step:7010/10262, training_loss:18.319733\n",
      "Step:7020/10262, training_loss:36.663429\n",
      "Step:7030/10262, training_loss:34.408443\n",
      "Step:7040/10262, training_loss:23.843985\n",
      "Step:7050/10262, training_loss:27.614853\n",
      "Step:7060/10262, training_loss:33.372009\n",
      "Step:7070/10262, training_loss:29.662867\n",
      "Step:7080/10262, training_loss:37.604248\n",
      "Step:7090/10262, training_loss:51.410389\n",
      "Step:7100/10262, training_loss:28.433760\n",
      "Step:7110/10262, training_loss:26.650700\n",
      "Step:7120/10262, training_loss:32.040276\n",
      "Step:7130/10262, training_loss:33.268185\n",
      "Step:7140/10262, training_loss:30.539886\n",
      "Step:7150/10262, training_loss:26.281593\n",
      "Step:7160/10262, training_loss:40.402256\n",
      "Step:7170/10262, training_loss:29.859894\n",
      "Step:7180/10262, training_loss:29.620543\n",
      "Step:7190/10262, training_loss:27.469547\n",
      "Step:7200/10262, training_loss:22.516005\n",
      "Step:7210/10262, training_loss:29.046228\n",
      "Step:7220/10262, training_loss:31.458925\n",
      "Step:7230/10262, training_loss:21.758469\n",
      "Step:7240/10262, training_loss:34.775494\n",
      "Step:7250/10262, training_loss:40.262871\n",
      "Step:7260/10262, training_loss:33.147717\n",
      "Step:7270/10262, training_loss:33.597519\n",
      "Step:7280/10262, training_loss:35.465630\n",
      "Step:7290/10262, training_loss:30.363087\n",
      "Step:7300/10262, training_loss:22.045252\n",
      "Step:7310/10262, training_loss:22.755562\n",
      "Step:7320/10262, training_loss:16.565308\n",
      "Step:7330/10262, training_loss:25.860041\n",
      "Step:7340/10262, training_loss:37.531921\n",
      "Step:7350/10262, training_loss:17.497982\n",
      "Step:7360/10262, training_loss:27.176365\n",
      "Step:7370/10262, training_loss:32.363472\n",
      "Step:7380/10262, training_loss:29.748682\n",
      "Step:7390/10262, training_loss:37.098877\n",
      "Step:7400/10262, training_loss:21.000504\n",
      "Step:7410/10262, training_loss:29.558216\n",
      "Step:7420/10262, training_loss:25.354675\n",
      "Step:7430/10262, training_loss:27.394939\n",
      "Step:7440/10262, training_loss:31.441807\n",
      "Step:7450/10262, training_loss:20.371075\n",
      "Step:7460/10262, training_loss:32.432587\n",
      "Step:7470/10262, training_loss:26.606253\n",
      "Step:7480/10262, training_loss:26.087900\n",
      "Step:7490/10262, training_loss:29.043112\n",
      "Step:7500/10262, training_loss:41.271187\n",
      "Step:7510/10262, training_loss:27.377707\n",
      "Step:7520/10262, training_loss:19.883030\n",
      "Step:7530/10262, training_loss:32.169067\n",
      "Step:7540/10262, training_loss:34.501221\n",
      "Step:7550/10262, training_loss:21.407650\n",
      "Step:7560/10262, training_loss:30.584423\n",
      "Step:7570/10262, training_loss:31.257366\n",
      "Step:7580/10262, training_loss:23.230860\n",
      "Step:7590/10262, training_loss:22.444105\n",
      "Step:7600/10262, training_loss:32.429749\n",
      "Step:7610/10262, training_loss:20.723499\n",
      "Step:7620/10262, training_loss:25.182850\n",
      "Step:7630/10262, training_loss:26.667328\n",
      "Step:7640/10262, training_loss:21.287468\n",
      "Step:7650/10262, training_loss:22.640903\n",
      "Step:7660/10262, training_loss:41.355618\n",
      "Step:7670/10262, training_loss:38.304008\n",
      "Step:7680/10262, training_loss:27.771595\n",
      "Step:7690/10262, training_loss:27.757271\n",
      "Step:7700/10262, training_loss:27.015484\n",
      "Step:7710/10262, training_loss:17.833130\n",
      "Step:7720/10262, training_loss:23.335218\n",
      "Step:7730/10262, training_loss:9.094051\n",
      "Step:7740/10262, training_loss:24.380478\n",
      "Step:7750/10262, training_loss:31.580666\n",
      "Step:7760/10262, training_loss:20.182547\n",
      "Step:7770/10262, training_loss:20.343349\n",
      "Step:7780/10262, training_loss:28.558067\n",
      "Step:7790/10262, training_loss:26.974976\n",
      "Step:7800/10262, training_loss:27.659012\n",
      "Step:7810/10262, training_loss:17.103176\n",
      "Step:7820/10262, training_loss:26.584791\n",
      "Step:7830/10262, training_loss:29.217726\n",
      "Step:7840/10262, training_loss:20.492273\n",
      "Step:7850/10262, training_loss:27.739765\n",
      "Step:7860/10262, training_loss:20.190907\n",
      "Step:7870/10262, training_loss:32.969887\n",
      "Step:7880/10262, training_loss:25.185467\n",
      "Step:7890/10262, training_loss:24.730125\n",
      "Step:7900/10262, training_loss:24.855164\n",
      "Step:7910/10262, training_loss:29.261831\n",
      "Step:7920/10262, training_loss:30.959892\n",
      "Step:7930/10262, training_loss:19.720230\n",
      "Step:7940/10262, training_loss:33.451973\n",
      "Step:7950/10262, training_loss:35.534378\n",
      "Step:7960/10262, training_loss:18.197563\n",
      "Step:7970/10262, training_loss:32.502029\n",
      "Step:7980/10262, training_loss:20.870941\n",
      "Step:7990/10262, training_loss:24.415081\n",
      "Step:8000/10262, training_loss:26.335489\n",
      "Step:8010/10262, training_loss:35.681038\n",
      "Step:8020/10262, training_loss:20.552032\n",
      "Step:8030/10262, training_loss:14.936651\n",
      "Step:8040/10262, training_loss:17.042559\n",
      "Step:8050/10262, training_loss:17.140274\n",
      "Step:8060/10262, training_loss:22.317989\n",
      "Step:8070/10262, training_loss:31.367964\n",
      "Step:8080/10262, training_loss:39.571823\n",
      "Step:8090/10262, training_loss:25.743717\n",
      "Step:8100/10262, training_loss:20.524467\n",
      "Step:8110/10262, training_loss:28.470364\n",
      "Step:8120/10262, training_loss:14.659327\n",
      "Step:8130/10262, training_loss:20.695875\n",
      "Step:8140/10262, training_loss:10.189300\n",
      "Step:8150/10262, training_loss:28.238997\n",
      "Step:8160/10262, training_loss:23.894417\n",
      "Step:8170/10262, training_loss:18.827415\n",
      "Step:8180/10262, training_loss:13.545754\n",
      "Step:8190/10262, training_loss:28.307777\n",
      "Step:8200/10262, training_loss:22.590263\n",
      "Step:8210/10262, training_loss:17.646681\n",
      "Step:8220/10262, training_loss:14.080894\n",
      "Step:8230/10262, training_loss:32.209469\n",
      "Step:8240/10262, training_loss:24.762358\n",
      "Step:8250/10262, training_loss:21.142151\n",
      "Step:8260/10262, training_loss:27.469944\n",
      "Step:8270/10262, training_loss:16.182201\n",
      "Step:8280/10262, training_loss:24.476616\n",
      "Step:8290/10262, training_loss:21.022579\n",
      "Step:8300/10262, training_loss:25.972805\n",
      "Step:8310/10262, training_loss:23.059347\n",
      "Step:8320/10262, training_loss:24.065035\n",
      "Step:8330/10262, training_loss:25.454460\n",
      "Step:8340/10262, training_loss:26.410654\n",
      "Step:8350/10262, training_loss:24.648281\n",
      "Step:8360/10262, training_loss:29.490345\n",
      "Step:8370/10262, training_loss:16.583698\n",
      "Step:8380/10262, training_loss:31.786247\n",
      "Step:8390/10262, training_loss:24.682924\n",
      "Step:8400/10262, training_loss:30.681850\n",
      "Step:8410/10262, training_loss:26.626461\n",
      "Step:8420/10262, training_loss:32.962830\n",
      "Step:8430/10262, training_loss:21.759159\n",
      "Step:8440/10262, training_loss:12.290859\n",
      "Step:8450/10262, training_loss:18.092655\n",
      "Step:8460/10262, training_loss:17.231079\n",
      "Step:8470/10262, training_loss:28.164583\n",
      "Step:8480/10262, training_loss:17.239477\n",
      "Step:8490/10262, training_loss:33.113235\n",
      "Step:8500/10262, training_loss:23.958897\n",
      "Step:8510/10262, training_loss:20.279131\n",
      "Step:8520/10262, training_loss:29.756418\n",
      "Step:8530/10262, training_loss:12.700779\n",
      "Step:8540/10262, training_loss:19.920233\n",
      "Step:8550/10262, training_loss:10.570146\n",
      "Step:8560/10262, training_loss:30.305885\n",
      "Step:8570/10262, training_loss:19.869019\n",
      "Step:8580/10262, training_loss:16.239273\n",
      "Step:8590/10262, training_loss:11.583176\n",
      "Step:8600/10262, training_loss:24.720463\n",
      "Step:8610/10262, training_loss:21.944401\n",
      "Step:8620/10262, training_loss:22.079149\n",
      "Step:8630/10262, training_loss:25.206892\n",
      "Step:8640/10262, training_loss:34.118019\n",
      "Step:8650/10262, training_loss:22.146824\n",
      "Step:8660/10262, training_loss:31.054482\n",
      "Step:8670/10262, training_loss:24.766911\n",
      "Step:8680/10262, training_loss:20.582277\n",
      "Step:8690/10262, training_loss:15.896466\n",
      "Step:8700/10262, training_loss:22.518530\n",
      "Step:8710/10262, training_loss:27.109961\n",
      "Step:8720/10262, training_loss:24.600069\n",
      "Step:8730/10262, training_loss:21.708927\n",
      "Step:8740/10262, training_loss:19.192654\n",
      "Step:8750/10262, training_loss:28.314671\n",
      "Step:8760/10262, training_loss:19.851128\n",
      "Step:8770/10262, training_loss:19.111691\n",
      "Step:8780/10262, training_loss:21.831293\n",
      "Step:8790/10262, training_loss:29.459747\n",
      "Step:8800/10262, training_loss:28.135021\n",
      "Step:8810/10262, training_loss:31.136276\n",
      "Step:8820/10262, training_loss:25.350737\n",
      "Step:8830/10262, training_loss:25.349979\n",
      "Step:8840/10262, training_loss:18.631359\n",
      "Step:8850/10262, training_loss:16.587074\n",
      "Step:8860/10262, training_loss:20.148794\n",
      "Step:8870/10262, training_loss:17.858305\n",
      "Step:8880/10262, training_loss:31.344151\n",
      "Step:8890/10262, training_loss:10.451975\n",
      "Step:8900/10262, training_loss:27.103251\n",
      "Step:8910/10262, training_loss:22.204117\n",
      "Step:8920/10262, training_loss:23.503218\n",
      "Step:8930/10262, training_loss:29.031027\n",
      "Step:8940/10262, training_loss:11.995417\n",
      "Step:8950/10262, training_loss:19.976334\n",
      "Step:8960/10262, training_loss:10.753269\n",
      "Step:8970/10262, training_loss:28.565744\n",
      "Step:8980/10262, training_loss:20.649969\n",
      "Step:8990/10262, training_loss:14.017145\n",
      "Step:9000/10262, training_loss:17.651968\n",
      "Step:9010/10262, training_loss:22.286629\n",
      "Step:9020/10262, training_loss:20.746357\n",
      "Step:9030/10262, training_loss:27.825872\n",
      "Step:9040/10262, training_loss:38.011696\n",
      "Step:9050/10262, training_loss:21.167034\n",
      "Step:9060/10262, training_loss:19.982615\n",
      "Step:9070/10262, training_loss:25.369186\n",
      "Step:9080/10262, training_loss:22.976545\n",
      "Step:9090/10262, training_loss:19.698824\n",
      "Step:9100/10262, training_loss:16.469090\n",
      "Step:9110/10262, training_loss:29.632324\n",
      "Step:9120/10262, training_loss:22.184238\n",
      "Step:9130/10262, training_loss:22.217209\n",
      "Step:9140/10262, training_loss:19.742464\n",
      "Step:9150/10262, training_loss:14.770490\n",
      "Step:9160/10262, training_loss:24.146759\n",
      "Step:9170/10262, training_loss:20.339890\n",
      "Step:9180/10262, training_loss:15.727694\n",
      "Step:9190/10262, training_loss:24.495651\n",
      "Step:9200/10262, training_loss:30.218821\n",
      "Step:9210/10262, training_loss:24.831423\n",
      "Step:9220/10262, training_loss:24.852699\n",
      "Step:9230/10262, training_loss:25.781311\n",
      "Step:9240/10262, training_loss:23.140516\n",
      "Step:9250/10262, training_loss:16.317863\n",
      "Step:9260/10262, training_loss:13.721514\n",
      "Step:9270/10262, training_loss:13.370732\n",
      "Step:9280/10262, training_loss:17.608036\n",
      "Step:9290/10262, training_loss:30.970869\n",
      "Step:9300/10262, training_loss:6.994929\n",
      "Step:9310/10262, training_loss:21.422359\n",
      "Step:9320/10262, training_loss:21.229576\n",
      "Step:9330/10262, training_loss:20.613844\n",
      "Step:9340/10262, training_loss:25.788273\n",
      "Step:9350/10262, training_loss:12.566898\n",
      "Step:9360/10262, training_loss:21.124397\n",
      "Step:9370/10262, training_loss:16.731401\n",
      "Step:9380/10262, training_loss:24.008297\n",
      "Step:9390/10262, training_loss:22.772781\n",
      "Step:9400/10262, training_loss:13.350266\n",
      "Step:9410/10262, training_loss:23.645523\n",
      "Step:9420/10262, training_loss:20.104168\n",
      "Step:9430/10262, training_loss:19.317406\n",
      "Step:9440/10262, training_loss:21.437199\n",
      "Step:9450/10262, training_loss:31.225363\n",
      "Step:9460/10262, training_loss:16.260876\n",
      "Step:9470/10262, training_loss:14.642238\n",
      "Step:9480/10262, training_loss:22.720114\n",
      "Step:9490/10262, training_loss:23.532856\n",
      "Step:9500/10262, training_loss:11.086622\n",
      "Step:9510/10262, training_loss:22.423788\n",
      "Step:9520/10262, training_loss:26.009109\n",
      "Step:9530/10262, training_loss:16.534801\n",
      "Step:9540/10262, training_loss:16.558739\n",
      "Step:9550/10262, training_loss:21.171116\n",
      "Step:9560/10262, training_loss:13.214747\n",
      "Step:9570/10262, training_loss:19.652184\n",
      "Step:9580/10262, training_loss:19.899807\n",
      "Step:9590/10262, training_loss:14.229816\n",
      "Step:9600/10262, training_loss:16.549591\n",
      "Step:9610/10262, training_loss:35.480095\n",
      "Step:9620/10262, training_loss:28.198441\n",
      "Step:9630/10262, training_loss:17.769798\n",
      "Step:9640/10262, training_loss:22.520021\n",
      "Step:9650/10262, training_loss:19.923183\n",
      "Step:9660/10262, training_loss:12.763514\n",
      "Step:9670/10262, training_loss:15.676796\n",
      "Step:9680/10262, training_loss:5.920522\n",
      "Step:9690/10262, training_loss:16.971516\n",
      "Step:9700/10262, training_loss:26.419323\n",
      "Step:9710/10262, training_loss:11.604291\n",
      "Step:9720/10262, training_loss:14.046227\n",
      "Step:9730/10262, training_loss:19.502743\n",
      "Step:9740/10262, training_loss:17.999519\n",
      "Step:9750/10262, training_loss:23.091505\n",
      "Step:9760/10262, training_loss:12.327822\n",
      "Step:9770/10262, training_loss:21.850296\n",
      "Step:9780/10262, training_loss:18.053104\n",
      "Step:9790/10262, training_loss:15.834758\n",
      "Step:9800/10262, training_loss:21.684015\n",
      "Step:9810/10262, training_loss:12.451962\n",
      "Step:9820/10262, training_loss:26.838440\n",
      "Step:9830/10262, training_loss:18.461706\n",
      "Step:9840/10262, training_loss:18.642365\n",
      "Step:9850/10262, training_loss:17.102827\n",
      "Step:9860/10262, training_loss:21.999866\n",
      "Step:9870/10262, training_loss:25.391590\n",
      "Step:9880/10262, training_loss:13.271345\n",
      "Step:9890/10262, training_loss:23.220140\n",
      "Step:9900/10262, training_loss:26.948891\n",
      "Step:9910/10262, training_loss:11.584801\n",
      "Step:9920/10262, training_loss:25.104261\n",
      "Step:9930/10262, training_loss:16.935379\n",
      "Step:9940/10262, training_loss:18.168827\n",
      "Step:9950/10262, training_loss:19.020901\n",
      "Step:9960/10262, training_loss:27.584295\n",
      "Step:9970/10262, training_loss:14.398126\n",
      "Step:9980/10262, training_loss:11.650286\n",
      "Step:9990/10262, training_loss:13.466576\n",
      "Step:10000/10262, training_loss:10.019448\n",
      "Step:10010/10262, training_loss:15.477221\n",
      "Step:10020/10262, training_loss:28.508287\n",
      "Step:10030/10262, training_loss:32.604729\n",
      "Step:10040/10262, training_loss:17.623367\n",
      "Step:10050/10262, training_loss:16.978357\n",
      "Step:10060/10262, training_loss:19.514330\n",
      "Step:10070/10262, training_loss:10.076896\n",
      "Step:10080/10262, training_loss:16.648827\n",
      "Step:10090/10262, training_loss:6.295811\n",
      "Step:10100/10262, training_loss:17.508190\n",
      "Step:10110/10262, training_loss:19.534531\n",
      "Step:10120/10262, training_loss:13.139967\n",
      "Step:10130/10262, training_loss:8.499385\n",
      "Step:10140/10262, training_loss:19.757984\n",
      "Step:10150/10262, training_loss:14.744072\n",
      "Step:10160/10262, training_loss:13.887697\n",
      "Step:10170/10262, training_loss:9.697519\n",
      "Step:10180/10262, training_loss:24.199612\n",
      "Step:10190/10262, training_loss:15.861843\n",
      "Step:10200/10262, training_loss:12.384254\n",
      "Step:10210/10262, training_loss:22.071442\n",
      "Step:10220/10262, training_loss:11.373732\n",
      "Step:10230/10262, training_loss:19.555609\n",
      "Step:10240/10262, training_loss:14.802687\n",
      "Step:10250/10262, training_loss:18.461901\n",
      "Step:10260/10262, training_loss:15.813876\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "!python3 gen_lyrics.py 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Sampling--\n",
      "Vocabulary Size:  2636\n",
      "./logs/lyrics_model.ckpt-10261\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "(\n",
      "\n",
      " mu~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ""
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "!python3 gen_lyrics.py 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
